# MLX Backend Changelog

## [1.0.0] - 2024-08-14
### Initial Release
- MLX framework integration for Apple Silicon
- Optimized for M1/M2/M3 chips
- Support for 4-bit and 8-bit quantized models
- Fast inference with Metal Performance Shaders
- OpenAI-compatible API
- Streaming responses
- Model caching

### Supported Models
- Llama 3.2 1B/3B Instruct
- Mistral 7B Instruct
- Phi-3 Mini
- Gemma 2B/7B
- All mlx-community models

### Requirements
- macOS 13.0+
- Apple Silicon (M1/M2/M3)
- Python 3.9+